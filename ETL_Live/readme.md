Below is a clean **README.md** you can use for your ETL Weather Pipeline.
It is fully aligned with your files:


# **Weather ETL Pipeline using Open-Meteo API & Supabase**

This project implements a complete **ETL (Extractâ€“Transformâ€“Load) pipeline** for weather data using the **Open-Meteo API**, processing hourly weather metrics for Hyderabad and inserting them into a **Supabase PostgreSQL database**.

---

# ğŸ“Œ **Pipeline Overview**

```
Extract â†’ Transform â†’ Load â†’ Supabase
```

### âœ” Extract

Fetch hourly weather data from **Open-Meteo API** and store it as raw JSON.

### âœ” Transform

Clean, standardize, and convert the JSON into a tabular CSV.

### âœ” Load

Insert final clean weather data into the **weather_data** table in Supabase.

---

# ğŸ“ **Project Folder Structure**

```
ETL_Weather/
â”‚
â”œâ”€â”€ Scripts/
â”‚   â”œâ”€â”€ Extractweather.py        # Extract step
â”‚   â”œâ”€â”€ Tr_weather.py            # Transform step
â”‚   â””â”€â”€ Loadweather.py           # Load step
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Raw JSON files (extracted)
â”‚   â””â”€â”€ staged/                  # Clean CSV (transformed)
â”‚
â””â”€â”€ README.md
```

---

# ğŸ” **1. Extract Step**

### File: `Extractweather.py`

Extracts live weather data from Open-Meteo API.

Uses:

* temperature
* humidity
* wind speed
* timestamps

Each run saves a file like:

```
data/raw/weather_YYMMDD_HHMMSS.json
```

Sample raw file: 

---

# ğŸ”„ **2. Transform Step**

### File: `Tr_weather.py`

Transforms the **latest** raw JSON file into a clean DataFrame.

Source fields â†’ Target fields:

| Raw Key               | Final Column      |
| --------------------- | ----------------- |
| temperature_2m        | temperature_c     |
| relative_humidity_2m  | humidity_percent  |
| wind_speed_10m        | wind_speed_kmph   |
| time                  | time              |
| + city added manually | Hyderabad         |
| extracted_at          | current timestamp |

Saves output:

```
data/staged/weather_cleaned.csv
```

Implementation: 

---

# ğŸ“¥ **3. Load Step**

### File: `Loadweather.py`

Loads the cleaned CSV into the Supabase table:

```
weather_data
```

Table columns:

```sql
id BIGSERIAL PRIMARY KEY,
time TIMESTAMP,
temperature_c DOUBLE PRECISION,
humidity_percent DOUBLE PRECISION,
city TEXT,
extracted_at TIMESTAMP,
wind_speed_kmph DOUBLE PRECISION
```

This step:

âœ” Converts timestamps to string ISO format
âœ” Renames CSV fields to match DB columns
âœ” Inserts in batches of 20
âœ” Sleeps to avoid Supabase rate limiting

Implementation: 

---

# ğŸ—„ **Supabase Weather Table Schema**

```sql
CREATE TABLE weather_data(
    id BIGSERIAL PRIMARY KEY,
    time TIMESTAMP,
    temperature_c DOUBLE PRECISION,
    humidity_percent DOUBLE PRECISION,
    city TEXT,
    extracted_at TIMESTAMP
);

ALTER TABLE weather_data
ADD COLUMN wind_speed_kmph DOUBLE PRECISION;
```

---

# ğŸš€ **How to Run the ETL**

### Step 1 â€” Extract

```
python Scripts/Extractweather.py
```

### Step 2 â€” Transform

```
python Scripts/Tr_weather.py
```

### Step 3 â€” Load

```
python Scripts/Loadweather.py
```

All done! Your Supabase table will now contain up-to-date hourly weather data.

---

# âš  Requirements

Install dependencies:

```
pip install requests pandas supabase python-dotenv
```

---

# ğŸ¯ **Future Enhancements**

* Automate ETL using cron / Windows Task Scheduler
* Store city dynamically
* Log processing steps
* Add error-handling and retry logic
* Support multiple cities

---


